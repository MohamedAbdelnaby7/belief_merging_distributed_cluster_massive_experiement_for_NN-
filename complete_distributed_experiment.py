#!/usr/bin/env python3
"""
Complete Standalone Distributed Belief Merging Experiment
Modified to support multiple grid sizes, agent numbers, and NEW MERGE METHODS.
Includes all original classes + distributed execution framework
No external dependencies on your original file
FIXED: Consistent seed generation for proper checkpointing
NOW USING MCTS INSTEAD OF MPC FOR PLANNING
UPDATED: Using Gurobi for Optimization instead of Scipy
FIXED: Gurobi Threading & Memory Management to prevent OOM
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
import json
import time
import pickle
import hashlib
from datetime import datetime
import os
import sys
from pathlib import Path
import logging
from scipy.stats import entropy, pearsonr, spearmanr
# from scipy.optimize import minimize # Scipy minimize replaced by Gurobi
import pandas as pd
import seaborn as sns
from typing import Dict, List, Tuple, Any, Union, Optional
import itertools
import warnings
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import psutil
import signal
import fcntl
import tempfile
import shutil
from dataclasses import dataclass, asdict, field
import argparse
import socket
import subprocess
import math
import gc  # Added for memory management

# Gurobi Import
try:
    import gurobipy as gp
    from gurobipy import GRB
except ImportError:
    print("WARNING: gurobipy not found. Optimization methods will fall back to analytical solutions.")

warnings.filterwarnings('ignore')


# ===================================================================
# SEED FIX: Consistent seed generation for proper checkpointing
# ===================================================================

def generate_consistent_seed(grid_size, n_agents, pattern, trial_id):
    """
    Generate a consistent seed that will be the same across Python sessions
    Replaces the problematic hash() function with hashlib for consistency
    
    Args:
        grid_size: (rows, cols) tuple
        n_agents: number of agents
        pattern: movement pattern string
        trial_id: trial identifier
    
    Returns:
        Consistent integer seed
    """
    seed_string = f"{grid_size[0]}x{grid_size[1]}_{n_agents}agents_{pattern}_trial{trial_id}"
    hash_object = hashlib.sha256(seed_string.encode())
    hash_hex = hash_object.hexdigest()
    seed = int(hash_hex[:8], 16) % (2**31 - 1)  # Keep it within int32 range
    return seed


# ===================================================================
# ORIGINAL BELIEF MERGING CLASSES
# ===================================================================

class UnifiedBeliefMergingFramework:
    """
    Framework for different belief merging approaches
    """
    def __init__(self, grid_size=(20, 20), n_agents=4):
        self.grid_size = grid_size
        self.total_states = grid_size[0] * grid_size[1]
        self.n_agents = n_agents
        
    def merge_beliefs_average(self, beliefs, agent_weights=None):
        """Simple averaging of beliefs (Arithmetic Mean) - Analytical"""
        if agent_weights is None:
            agent_weights = np.ones(len(beliefs)) / len(beliefs)
        
        merged = np.zeros_like(beliefs[0])
        for i, belief in enumerate(beliefs):
            merged += agent_weights[i] * belief
        
        return merged / np.sum(merged)

    def merge_beliefs_geometric(self, beliefs, agent_weights=None):
        """Geometric Mean (Logarithmic Opinion Pool) - Analytical"""
        # This corresponds to Reverse KL Minimization (Analytical Solution)
        if len(beliefs) == 1: return beliefs[0].copy()
        
        # Log-space summation to avoid underflow
        # log(prod(p_i)) = sum(log(p_i))
        log_beliefs = [np.log(np.clip(b, 1e-10, 1)) for b in beliefs]
        
        # If weights are provided, multiply logs by weights (weighted geometric mean)
        if agent_weights is not None:
             weighted_log_sum = np.sum([w * lb for w, lb in zip(agent_weights, log_beliefs)], axis=0)
        else:
             weighted_log_sum = np.sum(log_beliefs, axis=0)
             
        merged = np.exp(weighted_log_sum)
        return merged / np.sum(merged)
    
    def merge_beliefs_kl(self, beliefs, agent_weights=None):
        """KL divergence-based merging (Standard/Forward KL)"""
        # Note: Mathematically, min Sum w_i KL(P_i || Q) results in the Arithmetic Mean.
        # This optimization method is slower but effectively finds the same result as merge_beliefs_average.
        # We keep it for consistency with previous experiments.
        # Memory-optimized Forward KL using Gurobi
        
        if agent_weights is None:
            agent_weights = np.ones(len(beliefs))
        
        n_states = beliefs[0].shape[0]
        # Calculate linear coefficients C[x] = Sum_i w_i P_i(x)
        beliefs_flat = [b.flatten() for b in beliefs]
        C = np.zeros(n_states)
        for i, b in enumerate(beliefs_flat):
            C += agent_weights[i] * b
            
        try:
            # Gurobi Environment context manager to suppress output and handle licensing
            with gp.Env(empty=True) as env:
                env.setParam('OutputFlag', 0)
                env.setParam('Threads', 1) # CRITICAL: Prevent thread explosion
                env.setParam('MemLimit', 4) # Limit this specific optimization to 4GB
                env.start()
                with gp.Model("forward_kl", env=env) as model:
                    
                    # Variables Q(x)
                    q = model.addVars(n_states, lb=1e-9, ub=1.0, name="q")
                    
                    # Variables for log(Q(x))
                    log_q = model.addVars(n_states, lb=-float('inf'), name="log_q")
                    
                    # Constraint: Sum Q(x) = 1
                    model.addConstr(q.sum() == 1, "sum_prob")
                    
                    # General Constraints: log_q[i] = ln(q[i])
                    for i in range(n_states):
                        model.addGenConstrLog(q[i], log_q[i])
                        
                    # Objective: Maximize Sum C[i] * log_q[i]
                    obj_expr = gp.LinExpr()
                    for i in range(n_states):
                        if C[i] > 1e-12: # Skip negligible terms
                            obj_expr += C[i] * log_q[i]
                    
                    model.setObjective(obj_expr, GRB.MAXIMIZE)
                    model.optimize()
                    
                    if model.status == GRB.OPTIMAL:
                        merged = np.array([q[i].X for i in range(n_states)])
                        # Normalize to be safe, though constraint handles it
                        return merged.reshape(beliefs[0].shape) / np.sum(merged)
                    else:
                        # Fallback to analytical arithmetic mean if solver fails
                        return self.merge_beliefs_average(beliefs, agent_weights)
                        
        except Exception as e:
            # Fallback if Gurobi fails or not installed
            print(f"Gurobi Optimization Failed: {e}. Falling back to Arithmetic Mean.")
            return self.merge_beliefs_average(beliefs, agent_weights)

    def merge_beliefs_reverse_kl(self, beliefs, agent_weights=None):
        """Reverse KL divergence merging (Optimization based) using Gurobi"""
        # Objective: min Sum w_i KL(Q || P_i)
        # Equivalent to: min Sum_x Q(x) log Q(x) - Sum_x Q(x) * (Sum_i w_i log P_i(x))
        # The term x log x (negative entropy) is convex.
        
        if agent_weights is None:
            agent_weights = np.ones(len(beliefs))
            
        n_states = beliefs[0].shape[0]
        beliefs_flat = [b.flatten() for b in beliefs]
        
        # Calculate D[x] = Sum_i w_i log P_i(x)
        D = np.zeros(n_states)
        for i, b in enumerate(beliefs_flat):
            b_safe = np.clip(b, 1e-12, 1.0)
            D += agent_weights[i] * np.log(b_safe)
            
        try:
            with gp.Env(empty=True) as env:
                env.setParam('OutputFlag', 0)
                env.setParam('Threads', 1) # CRITICAL: Prevent thread explosion
                env.setParam('MemLimit', 4)
                env.start()
                with gp.Model("reverse_kl", env=env) as model:
                    
                    # Variable Q(x)
                    q = model.addVars(n_states, lb=1e-9, ub=1.0, name="q")
                    
                    # Constraint: Sum Q(x) = 1
                    model.addConstr(q.sum() == 1, "sum_prob")
                    
                    # Objective: Sum (q_i log q_i - D_i q_i)
                    # We use Piecewise Linear Approximation for x log x
                    # Define sampling points
                    x_pts = np.linspace(1e-9, 1.0, 100)
                    y_pts = x_pts * np.log(x_pts)
                    
                    for i in range(n_states):
                        # Combine convex entropy term and linear term
                        # cost = (q log q) - (D[i] * q)
                        y_pts_combined = y_pts - (D[i] * x_pts)
                        model.setPWLObj(q[i], x_pts, y_pts_combined)
                    
                    model.modelSense = GRB.MINIMIZE
                    model.optimize()
                    
                    if model.status == GRB.OPTIMAL:
                        merged = np.array([q[i].X for i in range(n_states)])
                        return merged.reshape(beliefs[0].shape) / np.sum(merged)
                    else:
                        # Fallback to analytical geometric mean
                        return self.merge_beliefs_geometric(beliefs, agent_weights)
                        
        except Exception as e:
            print(f"Gurobi Optimization Failed: {e}. Falling back to Geometric Mean.")
            return self.merge_beliefs_geometric(beliefs, agent_weights)
    
    def jensen_shannon_divergence(self, p, q):
        """Calculate Jensen-Shannon divergence between two distributions"""
        p = np.clip(p, 1e-10, 1)
        q = np.clip(q, 1e-10, 1)
        m = 0.5 * (p + q)
        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))


class TargetMovementPolicy:
    """
    Target movement policy - simple patterns without MPC/MDP
    """
    def __init__(self, grid_size, movement_pattern='random'):
        self.grid_size = grid_size
        self.rows, self.cols = grid_size
        self.movement_pattern = movement_pattern
        self.step_count = 0
        
    def get_next_position(self, current_pos, step=None):
        """Get next position based on movement pattern"""
        if step is not None:
            self.step_count = step
            
        r, c = divmod(current_pos, self.cols)
        
        if self.movement_pattern == 'random':
            # Random walk with 0.8 prob of moving, 0.2 of staying
            if np.random.random() < 0.2:
                return current_pos
                
            moves = []
            if r > 0: moves.append(current_pos - self.cols)
            if r < self.rows-1: moves.append(current_pos + self.cols)
            if c > 0: moves.append(current_pos - 1)
            if c < self.cols-1: moves.append(current_pos + 1)
            
            return np.random.choice(moves) if moves else current_pos
            
        elif self.movement_pattern == 'evasive':
            # Try to move away from center
            center_r, center_c = self.rows // 2, self.cols // 2
            
            # Calculate direction away from center
            dr = 1 if r > center_r else -1 if r < center_r else 0
            dc = 1 if c > center_c else -1 if c < center_c else 0
            
            # Preferred moves
            preferred = []
            if 0 <= r + dr < self.rows:
                preferred.append(current_pos + dr * self.cols)
            if 0 <= c + dc < self.cols:
                preferred.append(current_pos + dc)
                
            if preferred and np.random.random() < 0.7:
                return np.random.choice(preferred)
            else:
                # Random move
                moves = []
                if r > 0: moves.append(current_pos - self.cols)
                if r < self.rows-1: moves.append(current_pos + self.cols)
                if c > 0: moves.append(current_pos - 1)
                if c < self.cols-1: moves.append(current_pos + 1)
                return np.random.choice(moves) if moves else current_pos
                
        elif self.movement_pattern == 'patrol':
            # Circular patrol pattern
            corners = [
                0,  # top-left
                self.cols - 1,  # top-right
                (self.rows - 1) * self.cols + self.cols - 1,  # bottom-right
                (self.rows - 1) * self.cols  # bottom-left
            ]
            
            # Find closest corner
            min_dist = float('inf')
            target_corner = corners[0]
            for corner in corners:
                corner_r, corner_c = divmod(corner, self.cols)
                dist = abs(r - corner_r) + abs(c - corner_c)
                if dist < min_dist and dist > 0:  # Don't stay at current corner
                    min_dist = dist
                    target_corner = corner
                    
            # Move towards target corner
            target_r, target_c = divmod(target_corner, self.cols)
            dr = 1 if target_r > r else -1 if target_r < r else 0
            dc = 1 if target_c > c else -1 if target_c < c else 0
            
            new_r = r + dr
            new_c = c + dc
            
            if 0 <= new_r < self.rows and 0 <= new_c < self.cols:
                return new_r * self.cols + new_c
                
        return current_pos


# --- MCTS IMPLEMENTATION ---
class MCTSNode:
    """Node in MCTS Tree"""
    def __init__(self, state: Dict, parent=None, action=None):
        self.state = state  # {'beliefs': [np.array], 'positions': [int]}
        self.parent = parent
        self.action = action  # Joint action that led to this state
        self.children = []
        self.visits = 0
        self.value = 0.0
        self.untried_actions = None # Will be populated on expansion

    def is_fully_expanded(self):
        return self.untried_actions is not None and len(self.untried_actions) == 0

    def best_child(self, c_param=1.414):
        choices_weights = [
            (child.value / child.visits) + c_param * np.sqrt((2 * np.log(self.visits) / child.visits))
            for child in self.children
        ]
        return self.children[np.argmax(choices_weights)]

class MultiAgentMCTS:
    """
    MCTS implementation replacing MPC for faster joint action planning
    """
    def __init__(self, grid_size, n_agents, horizon=10, alpha=0.1, beta=0.2, simulations=50):
        self.grid_size = grid_size
        self.rows, self.cols = grid_size
        self.n_agents = n_agents
        self.horizon = horizon  # Max depth
        self.alpha = alpha
        self.beta = beta
        self.n_states = grid_size[0] * grid_size[1]
        self.simulations = simulations # Number of MCTS iterations per step
        # 8-Direction offsets (row_change, col_change) - No (0,0) allowed
        self.move_offsets = [
            (-1, 0), (1, 0), (0, -1), (0, 1),   # Cardinal
            (-1, -1), (-1, 1), (1, -1), (1, 1)  # Diagonal
        ]

    def get_joint_action(self, beliefs: Union[np.ndarray, List[np.ndarray]], 
                        agent_positions: List[int], 
                        fast_mode: bool = False, # Kept for API compatibility
                        random_walk_mode: bool = False) -> List[int]: 
        
        if random_walk_mode:
             return self._get_random_joint_action(agent_positions)

        # Root state
        if isinstance(beliefs, np.ndarray):
             root_beliefs = [beliefs.copy()] # Wrap single belief for consistency logic
             shared_mode = True
        else:
             root_beliefs = [b.copy() for b in beliefs]
             shared_mode = False

        root_state = {'beliefs': root_beliefs, 'positions': agent_positions}
        root = MCTSNode(root_state)
        
        # MCTS Loop
        for _ in range(self.simulations):
            node = root
            
            # Selection
            while node.children and node.is_fully_expanded():
                node = node.best_child()

            # Expansion
            if not node.children or not node.is_fully_expanded():
                if node.untried_actions is None:
                    node.untried_actions = self._get_legal_joint_actions(node.state['positions'])
                
                if node.untried_actions:
                    action = node.untried_actions.pop()
                    new_state = self._simulate_step(node.state, action, shared_mode)
                    child_node = MCTSNode(new_state, parent=node, action=action)
                    node.children.append(child_node)
                    node = child_node
            
            # Simulation (Rollout) - Estimate value from this state
            # For belief merging, value is usually negative entropy (Information Gain)
            # We do a shallow rollout or just eval current state for speed
            final_rollout_state = node.state # Simplification: Just eval state quality
            
            # Calculate Reward (Negative Entropy / Information Gain)
            reward = 0
            for b in final_rollout_state['beliefs']:
                reward += -entropy(b) # Maximize negative entropy = Minimize entropy
            
            # Backpropagation
            while node is not None:
                node.visits += 1
                node.value += reward
                node = node.parent
        
        # Pick best action (most visited child)
        if not root.children:
             # Fallback if no simulations worked (shouldn't happen)
             return agent_positions 
             
        best_child = max(root.children, key=lambda c: c.visits)
        return best_child.action

    def _get_legal_joint_actions(self, current_positions):
        """Generates a diverse subset of 8-direction joint moves"""
        agent_legal_moves = []
        for pos in current_positions:
            r, c = divmod(pos, self.grid_size[1])
            moves = []
            for dr, dc in self.move_offsets:
                nr, nc = r + dr, c + dc
                if 0 <= nr < self.grid_size[0] and 0 <= nc < self.grid_size[1]:
                    moves.append(nr * self.grid_size[1] + nc)
            agent_legal_moves.append(moves)
        
        # To avoid 8^n explosion, we sample 30-100 joint actions 
        # but ensure they are diverse (using random selection from Cartesian product)
        joint_actions = []
        for _ in range(100): 
            action = tuple(np.random.choice(m) for m in agent_legal_moves)
            joint_actions.append(action)
        return list(set(joint_actions))

    def _simulate_step(self, state, joint_action, shared_mode):
        """Apply action and simulated observation to get new state"""
        new_positions = list(joint_action)
        current_beliefs = [b.copy() for b in state['beliefs']]
        
        # Simulate Observations (Hypothetical)
        # To keep tree deterministic for a "planning" step, we usually assume 
        # observations consistent with current belief peak or just no observation update logic 
        # inside the tree expansion to save cost. 
        # BUT for Info Gain, we need belief update.
        # We will simulate "expected" update or just random sample based on belief.
        
        # Simplified: Update belief assuming NO detection (most common) or detection at highest prob?
        # Better: Sample outcome based on current belief.
        
        new_beliefs = []
        for i, b in enumerate(current_beliefs):
            # Pick a target loc based on belief
            # If shared mode, we use the single belief for simulation logic
            # If independent, we use the specific agent's belief
            
            if shared_mode:
                # One shared belief updated by ALL agents
                temp_belief = b.copy()
                for pos in new_positions:
                    # Sim single obs
                    # Assume target is at max prob loc for planning heuristic
                    target_assumed = np.argmax(temp_belief)
                    sim_obs = 1 if (pos == target_assumed and np.random.random() > self.beta) else 0
                    temp_belief = self._update_belief_single(temp_belief, pos, sim_obs)
                new_beliefs.append(temp_belief)
            else:
                # Independent
                 # Assume target is at max prob loc for planning heuristic
                target_assumed = np.argmax(b)
                pos = new_positions[i]
                sim_obs = 1 if (pos == target_assumed and np.random.random() > self.beta) else 0
                new_b = self._update_belief_single(b, pos, sim_obs)
                new_beliefs.append(new_b)
                
        return {'beliefs': new_beliefs, 'positions': new_positions}

    def _get_random_joint_action(self, agent_positions: List[int]) -> List[int]:
        """Simple random walk for all agents (Baseline)"""
        joint_action = []
        for pos in agent_positions:
            neighbors = self._get_neighbors(pos)
            joint_action.append(np.random.choice(neighbors))
        return joint_action

    def _update_belief_single(self, belief: np.ndarray, 
                            position: int, 
                            observation: int) -> np.ndarray:
        """
        Update single agent's belief (Copied from MPC for compatibility)
        """
        likelihood = np.ones(self.n_states)
        
        if observation == 1:  # Detection
            likelihood[position] = 1 - self.beta
            likelihood[np.arange(self.n_states) != position] = self.alpha
        else:  # No detection
            likelihood[position] = self.beta
            likelihood[np.arange(self.n_states) != position] = 1 - self.alpha
        
        # Bayesian update
        posterior = belief * likelihood
        return posterior / (np.sum(posterior) + 1e-10)
    
    def _get_neighbors(self, position: int) -> List[int]:
        """Get valid neighboring positions including current position"""
        r, c = divmod(position, self.cols)
        neighbors = [position]  # Include current position
        
        if r > 0:
            neighbors.append(position - self.cols)
        if r < self.rows - 1:
            neighbors.append(position + self.cols)
        if c > 0:
            neighbors.append(position - 1)
        if c < self.cols - 1:
            neighbors.append(position + 1)
            
        return neighbors


class ControlledMergingExperiment:
    """
    Main experiment class with proper MPC implementation
    """
    
    def __init__(self, grid_size=(20, 20), n_agents=4, alpha=0.1, beta=0.2, horizon=2):
        self.grid_size = grid_size
        self.n_agents = n_agents
        self.alpha = alpha  # False positive rate
        self.beta = beta   # False negative rate
        self.merger = UnifiedBeliefMergingFramework(grid_size, n_agents)
        # REPLACED MPC WITH MCTS
        self.planner = MultiAgentMCTS(grid_size, n_agents, horizon, alpha, beta, simulations=20) 
        
    def _run_single_experiment(self, trial_config, merge_interval, max_steps, fast_mode=False, 
                               random_walk_mode=False, merge_method='standard_kl'):
        """Run a single experiment with specified merge interval and method"""
        # Special case for full communication
        if merge_interval == 0:
            return self._run_centralized_full_communication(trial_config, max_steps, fast_mode, random_walk_mode)
        
        # For other strategies
        start_time = time.time()
        
        # Initialize agents with independent beliefs
        agent_beliefs = [
            np.ones(self.grid_size[0] * self.grid_size[1]) / (self.grid_size[0] * self.grid_size[1])
            for _ in range(self.n_agents)
        ]
        
        agent_positions = trial_config['initial_positions'].copy()
        agent_trajectories = [[pos] for pos in agent_positions]
        
        # Tracking variables
        target_found = False
        first_discovery_step = max_steps
        discovery_count = 0
        entropy_history = []
        divergence_history = []
        merge_events = []
        
        # Run simulation
        for step in range(max_steps):
            # Target position for this step
            target_pos = trial_config['target_trajectory'][step]
            
            # Calculate and store metrics BEFORE actions
            entropies = [entropy(b) for b in agent_beliefs]
            entropy_history.append({
                'mean': np.mean(entropies),
                'std': np.std(entropies),
                'max': np.max(entropies),
                'min': np.min(entropies) 
            })
            
            # Calculate divergence between agents
            divergences = []
            for i in range(len(agent_beliefs)):
                for j in range(i+1, len(agent_beliefs)):
                    div = self.merger.jensen_shannon_divergence(agent_beliefs[i], agent_beliefs[j])
                    divergences.append(div)
            
            divergence_history.append({
                'mean': np.mean(divergences) if divergences else 0,
                'std': np.std(divergences) if divergences else 0,
                'max': np.max(divergences) if divergences else 0
            })
            
            # Check if it's time to merge
            if merge_interval != float('inf') and step > 0 and step % merge_interval == 0:
                # Save beliefs BEFORE merge
                beliefs_before = [b.copy() for b in agent_beliefs]
                
                # Perform belief merging - UPDATED TO SUPPORT METHODS
                if merge_method == 'geometric_mean':
                    merged_belief = self.merger.merge_beliefs_geometric(agent_beliefs)
                elif merge_method == 'arithmetic_mean':
                    merged_belief = self.merger.merge_beliefs_average(agent_beliefs)
                elif merge_method == 'reverse_kl':
                    merged_belief = self.merger.merge_beliefs_reverse_kl(agent_beliefs)
                elif merge_method == 'standard_kl':
                    merged_belief = self.merger.merge_beliefs_kl(agent_beliefs)
                else: # Fallback to KL
                    merged_belief = self.merger.merge_beliefs_kl(agent_beliefs)
                
                # Update all agents with merged belief
                for i in range(self.n_agents):
                    agent_beliefs[i] = merged_belief.copy()
                
                # Calculate entropy change
                entropy_before = np.mean([entropy(b) for b in beliefs_before])
                entropy_after = entropy(merged_belief)
                
                merge_events.append({
                    'step': step,
                    'entropy_before': entropy_before,
                    'entropy_after': entropy_after,
                    'entropy_reduction': entropy_before - entropy_after
                })
            
            # Get joint action using MCTS (REPLACED MPC CALL)
            joint_action = self.planner.get_joint_action(
                agent_beliefs,
                agent_positions,
                fast_mode=fast_mode,
                random_walk_mode=random_walk_mode
            )
            
            # Make observations BEFORE moving
            for i, pos in enumerate(agent_positions):
                obs_rand = trial_config['observation_randoms'][step, i]
                
                if pos == target_pos:
                    observation = 1 if obs_rand > self.beta else 0
                    if observation == 1 and not target_found:
                        target_found = True
                        first_discovery_step = step
                    if observation == 1:
                        discovery_count += 1
                else:
                    observation = 1 if obs_rand < self.alpha else 0
                
                # Update individual belief using helper in planner
                agent_beliefs[i] = self.planner._update_belief_single(
                    agent_beliefs[i], pos, observation
                )
            
            # Execute joint action
            agent_positions = joint_action
            for i, new_pos in enumerate(joint_action):
                agent_trajectories[i].append(new_pos)
        
        # Final merge for no_comm strategy
        if merge_interval == float('inf'):
            final_beliefs = agent_beliefs
            # For final metric, we can use the requested method to see the theoretical consensus
            if merge_method == 'geometric_mean' or merge_method == 'reverse_kl':
                final_merged = self.merger.merge_beliefs_geometric(final_beliefs)
            else:
                final_merged = self.merger.merge_beliefs_kl(final_beliefs)
        else:
            final_beliefs = agent_beliefs
            final_merged = np.mean(final_beliefs, axis=0)
            final_merged = final_merged / np.sum(final_merged)
        
        # Calculate final metrics
        final_target_pos = trial_config['target_trajectory'][-1]
        
        # Performance metrics
        prob_at_true_target = final_merged[final_target_pos]
        
        # Find position with highest belief
        predicted_pos = np.argmax(final_merged)
        pred_r, pred_c = divmod(predicted_pos, self.grid_size[1])
        true_r, true_c = divmod(final_target_pos, self.grid_size[1])
        prediction_error = np.sqrt((pred_r - true_r)**2 + (pred_c - true_c)**2)
        
        elapsed_time = time.time() - start_time
        
        return {
            'target_found': target_found,
            'first_discovery_step': first_discovery_step,
            'discovery_count': discovery_count,
            'elapsed_time': elapsed_time,
            'final_merged_belief': final_merged,
            'final_entropy': entropy(final_merged),
            'entropy_history': entropy_history,
            'divergence_history': divergence_history,
            'merge_events': merge_events,
            'total_merges': len(merge_events),
            'prob_at_true_target': prob_at_true_target,
            'prediction_error': prediction_error,
            'agent_trajectories': agent_trajectories
        }
    
    def _run_centralized_full_communication(self, trial_config, max_steps, fast_mode=False, random_walk_mode=False):
        """Run true full communication - single shared belief, coordinated MPC"""
        start_time = time.time()
        
        # Single shared belief for all agents
        shared_belief = np.ones(self.grid_size[0] * self.grid_size[1]) / (self.grid_size[0] * self.grid_size[1])
        
        # Agent positions
        agent_positions = trial_config['initial_positions'].copy()
        agent_trajectories = [[pos] for pos in agent_positions]
        
        # Tracking variables
        target_found = False
        first_discovery_step = max_steps
        discovery_count = 0
        entropy_history = []
        
        # Add artificial communication overhead
        COMM_OVERHEAD_PER_STEP = 0.001 * self.n_agents * (self.n_agents - 1) / 2
        
        # Run simulation
        for step in range(max_steps):
            # Target position for this step
            target_pos = trial_config['target_trajectory'][step]
            
            # Calculate and store metrics
            entropy_val = entropy(shared_belief)
            entropy_history.append({
                'mean': entropy_val,
                'std': 0,  # No variance - single belief
                'max': entropy_val,
                'min': entropy_val
            })
            
            # Get joint action using MCTS with shared belief (REPLACED MPC)
            joint_action = self.planner.get_joint_action(
                shared_belief, 
                agent_positions, 
                fast_mode=fast_mode,
                random_walk_mode=random_walk_mode
            )
            
            # ALL agents make observations BEFORE moving
            for i, pos in enumerate(agent_positions):
                obs_rand = trial_config['observation_randoms'][step, i]
                
                if pos == target_pos:
                    observation = 1 if obs_rand > self.beta else 0
                    if observation == 1 and not target_found:
                        target_found = True
                        first_discovery_step = step
                    if observation == 1:
                        discovery_count += 1
                else:
                    observation = 1 if obs_rand < self.alpha else 0
                
                # Update SHARED belief using helper in planner
                shared_belief = self.planner._update_belief_single(
                    shared_belief, pos, observation
                )
            
            # Execute joint action
            agent_positions = joint_action
            for i, new_pos in enumerate(joint_action):
                agent_trajectories[i].append(new_pos)
            
            # Add communication overhead
            time.sleep(COMM_OVERHEAD_PER_STEP)
        
        # Final metrics
        final_target_pos = trial_config['target_trajectory'][-1]
        
        # Performance metrics
        prob_at_true_target = shared_belief[final_target_pos]
        
        # Find position with highest belief
        predicted_pos = np.argmax(shared_belief)
        pred_r, pred_c = divmod(predicted_pos, self.grid_size[1])
        true_r, true_c = divmod(final_target_pos, self.grid_size[1])
        prediction_error = np.sqrt((pred_r - true_r)**2 + (pred_c - true_c)**2)
        
        elapsed_time = time.time() - start_time
        
        return {
            'target_found': target_found,
            'first_discovery_step': first_discovery_step,
            'discovery_count': discovery_count,
            'elapsed_time': elapsed_time,
            'final_merged_belief': shared_belief,
            'final_entropy': entropy(shared_belief),
            'entropy_history': entropy_history,
            'divergence_history': [{'mean': 0, 'std': 0, 'max': 0} for _ in range(max_steps)],
            'merge_events': [],  # No merge events - always together
            'total_merges': 0,
            'prob_at_true_target': prob_at_true_target,
            'prediction_error': prediction_error,
            'agent_trajectories': agent_trajectories
        }


# ===================================================================
# DISTRIBUTED EXECUTION FRAMEWORK 
# ===================================================================

@dataclass
class ExperimentConfig:
    """Configuration for the experiment - now supports multiple grid sizes and agent numbers"""
    grid_sizes: List[Tuple[int, int]] = field(default_factory=lambda: [(20, 20)])
    n_agents_list: List[int] = field(default_factory=lambda: [4])
    alpha: float = 0.1  # False positive rate
    beta: float = 0.2   # False negative rate
    horizon: int = 2    # MPC/MCTS horizon
    n_trials: int = 30
    max_steps: int = 1000
    merge_intervals: List[Union[int, float]] = None
    target_patterns: List[str] = None
    fast_mode: bool = False  # TRUE Planner by default
    random_walk_mode: bool = False # Control mode: True = Random Walk, False = Active Planner
    merge_methods: List[str] = None # List of methods to test
    
    def __post_init__(self):
        if self.merge_intervals is None:
            self.merge_intervals = [0, 10, 25, 50, 100, 200, 500, float('inf')]
        if self.target_patterns is None:
            self.target_patterns = ['random', 'evasive', 'patrol']
        if self.merge_methods is None:
            self.merge_methods = ['standard_kl']
    
    def to_dict(self):
        d = asdict(self)
        # Convert tuples to lists for JSON serialization
        d['grid_sizes'] = [[r, c] for r, c in self.grid_sizes]
        return d
    
    @classmethod
    def from_dict(cls, d):
        # Convert lists back to tuples for grid sizes
        if 'grid_sizes' in d:
            d['grid_sizes'] = [(r, c) for r, c in d['grid_sizes']]
        return cls(**d)


@dataclass
class TrialTask:
    """Individual trial task for parallel execution - now includes grid size and n_agents"""
    grid_size: Tuple[int, int]
    n_agents: int
    pattern: str
    trial_id: int
    merge_interval: Union[int, float]
    merge_method: str
    config: ExperimentConfig
    trial_seed: int
    checkpoint_dir: str
    
    def get_task_id(self):
        """Generate unique task ID including grid size and n_agents"""
        grid_str = f"{self.grid_size[0]}x{self.grid_size[1]}"
        return f"grid{grid_str}_agents{self.n_agents}_{self.pattern}_{self.merge_method}_trial{self.trial_id}_interval{self.merge_interval}_seed{self.trial_seed}"
    
    def get_checkpoint_path(self):
        """Get checkpoint file path for this task"""
        return os.path.join(self.checkpoint_dir, f"{self.get_task_id()}.pkl")


class DistributedExperimentManager:
    """Manages distributed execution with checkpointing - now supports multiple configurations"""
    
    def __init__(self, config: ExperimentConfig, checkpoint_dir: str = "checkpoints", 
                 results_dir: str = "results", max_workers: int = None):
        self.config = config
        self.checkpoint_dir = Path(checkpoint_dir)
        self.results_dir = Path(results_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        
        # Determine optimal number of workers
        if max_workers is None:
            # Use 80% of available CPUs, but cap at 500 to avoid overwhelming
            max_workers = min(int(psutil.cpu_count() * 0.8), 500)
        self.max_workers = max_workers
        
        # Setup logging
        self.setup_logging()
        
        # Progress tracking
        self.total_tasks = 0
        self.completed_tasks = 0
        self.failed_tasks = 0
        
    def setup_logging(self):
        """Setup comprehensive logging"""
        log_dir = self.results_dir / "logs"
        log_dir.mkdir(exist_ok=True)
        
        # Main log file
        log_file = log_dir / f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Experiment started on {socket.gethostname()}")
        self.logger.info(f"Using {self.max_workers} workers")
        self.logger.info(f"Config: {self.config}")
    
    def generate_all_tasks(self) -> List[TrialTask]:
        """Generate all trial tasks for multiple grid sizes and agent numbers"""
        tasks = []
        
        for grid_size in self.config.grid_sizes:
            for n_agents in self.config.n_agents_list:
                for pattern in self.config.target_patterns:
                    for trial_id in range(self.config.n_trials):
                        # FIXED: Use consistent seed generation instead of hash()
                        trial_seed = generate_consistent_seed(grid_size, n_agents, pattern, trial_id)
                        
                        for merge_interval in self.config.merge_intervals:
                            for merge_method in self.config.merge_methods:
                                task = TrialTask(
                                    grid_size=grid_size,
                                    n_agents=n_agents,
                                    pattern=pattern,
                                    trial_id=trial_id,
                                    merge_interval=merge_interval,
                                    merge_method=merge_method,
                                    config=self.config,
                                    trial_seed=trial_seed,
                                    checkpoint_dir=str(self.checkpoint_dir)
                                )
                                tasks.append(task)
        
        self.total_tasks = len(tasks)
        self.logger.info(f"Generated {self.total_tasks} total tasks")
        self.logger.info(f"Grid sizes: {self.config.grid_sizes}")
        self.logger.info(f"Agent numbers: {self.config.n_agents_list}")
        return tasks
    
    def filter_incomplete_tasks(self, tasks: List[TrialTask]) -> List[TrialTask]:
        """Filter out already completed tasks"""
        incomplete_tasks = []
        
        for task in tasks:
            checkpoint_path = task.get_checkpoint_path()
            if not os.path.exists(checkpoint_path):
                incomplete_tasks.append(task)
            else:
                # Verify checkpoint integrity
                try:
                    with open(checkpoint_path, 'rb') as f:
                        result = pickle.load(f)
                    if self.validate_result(result):
                        self.completed_tasks += 1
                        continue
                    else:
                        self.logger.warning(f"Invalid checkpoint found: {checkpoint_path}")
                        os.remove(checkpoint_path)
                except Exception as e:
                    self.logger.warning(f"Corrupted checkpoint: {checkpoint_path}, error: {e}")
                    if os.path.exists(checkpoint_path):
                        os.remove(checkpoint_path)
                
                incomplete_tasks.append(task)
        
        self.logger.info(f"Found {self.completed_tasks} completed tasks")
        self.logger.info(f"Remaining tasks: {len(incomplete_tasks)}")
        return incomplete_tasks
    
    def validate_result(self, result: Dict) -> bool:
        """Validate that a result contains all required fields"""
        required_fields = [
            'target_found', 'first_discovery_step', 'discovery_count',
            'elapsed_time', 'final_merged_belief', 'final_entropy',
            'entropy_history', 'divergence_history', 'prediction_error'
        ]
        
        return all(field in result for field in required_fields)
    
    def run_distributed_experiment(self):
        """Run the complete distributed experiment"""
        self.logger.info("Starting distributed experiment")
        start_time = time.time()
        
        # Generate all tasks
        all_tasks = self.generate_all_tasks()
        
        # Filter out completed tasks
        remaining_tasks = self.filter_incomplete_tasks(all_tasks)
        
        if not remaining_tasks:
            self.logger.info("All tasks already completed!")
            return self.collect_results()
        
        # Setup signal handlers for graceful shutdown
        self.setup_signal_handlers()
        
        # Run tasks in parallel
        self.logger.info(f"Starting parallel execution with {self.max_workers} workers")
        
        try:
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                # Submit all tasks
                future_to_task = {
                    executor.submit(run_single_trial_task, task): task 
                    for task in remaining_tasks
                }
                
                # Process completed tasks
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    
                    try:
                        result = future.result()
                        if result is not None:
                            self.completed_tasks += 1
                            self.logger.info(
                                f"Completed {task.get_task_id()} "
                                f"({self.completed_tasks}/{self.total_tasks})"
                            )
                        else:
                            self.failed_tasks += 1
                            self.logger.error(f"Failed: {task.get_task_id()}")
                    
                    except Exception as e:
                        self.failed_tasks += 1
                        self.logger.error(f"Task {task.get_task_id()} failed: {e}")
                    
                    # Progress update
                    if self.completed_tasks % 50 == 0:
                        elapsed = time.time() - start_time
                        rate = self.completed_tasks / elapsed
                        eta = (self.total_tasks - self.completed_tasks) / rate if rate > 0 else 0
                        self.logger.info(
                            f"Progress: {self.completed_tasks}/{self.total_tasks} "
                            f"({100*self.completed_tasks/self.total_tasks:.1f}%), "
                            f"Rate: {rate:.2f} tasks/sec, ETA: {eta/3600:.1f} hours"
                        )
        
        except KeyboardInterrupt:
            self.logger.info("Received interrupt signal, shutting down gracefully...")
            return None
        
        total_time = time.time() - start_time
        self.logger.info(
            f"Experiment completed in {total_time/3600:.2f} hours. "
            f"Completed: {self.completed_tasks}, Failed: {self.failed_tasks}"
        )
        
        # Collect and analyze results
        return self.collect_results()
    
    def setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            self.logger.info(f"Received signal {signum}, initiating graceful shutdown...")
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def collect_results(self):
        """Collect all results from checkpoints - now organized by grid size and n_agents"""
        self.logger.info("Collecting results from checkpoints...")
        
        all_results = {}
        
        for grid_size in self.config.grid_sizes:
            grid_key = f"{grid_size[0]}x{grid_size[1]}"
            all_results[grid_key] = {}
            
            for n_agents in self.config.n_agents_list:
                agent_key = f"{n_agents}_agents"
                all_results[grid_key][agent_key] = {}
                
                for pattern in self.config.target_patterns:
                    pattern_results = {}
                    
                    for merge_interval in self.config.merge_intervals:
                        interval_results = {}
                        
                        # Initialize for methods
                        for method in self.config.merge_methods:
                            interval_results[method] = []
                        
                        for trial_id in range(self.config.n_trials):
                            # FIXED: Use consistent seed generation
                            trial_seed = generate_consistent_seed(grid_size, n_agents, pattern, trial_id)
                            
                            for merge_method in self.config.merge_methods:
                                task = TrialTask(
                                    grid_size=grid_size,
                                    n_agents=n_agents,
                                    pattern=pattern,
                                    trial_id=trial_id,
                                    merge_interval=merge_interval,
                                    merge_method=merge_method,
                                    config=self.config,
                                    trial_seed=trial_seed,
                                    checkpoint_dir=str(self.checkpoint_dir)
                                )
                                
                                checkpoint_path = task.get_checkpoint_path()
                                
                                if os.path.exists(checkpoint_path):
                                    try:
                                        with open(checkpoint_path, 'rb') as f:
                                            result = pickle.load(f)
                                        interval_results[merge_method].append(result)
                                    except Exception as e:
                                        self.logger.error(f"Failed to load {checkpoint_path}: {e}")
                        
                        # Store results with proper key naming
                        if merge_interval == 0:
                            key = 'full_comm'
                        elif merge_interval == float('inf'):
                            key = 'no_comm'
                        else:
                            key = f'interval_{merge_interval}'
                        
                        pattern_results[key] = interval_results
                    
                    all_results[grid_key][agent_key][pattern] = pattern_results
        
        # Save consolidated results
        consolidated_path = self.results_dir / f"consolidated_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        with open(consolidated_path, 'wb') as f:
            pickle.dump(all_results, f)
        
        self.logger.info(f"Consolidated results saved to {consolidated_path}")
        return all_results


def run_single_trial_task(task: TrialTask) -> Optional[Dict]:
    """Run a single trial task - designed for parallel execution"""
    
    # Check if already completed
    checkpoint_path = task.get_checkpoint_path()
    if os.path.exists(checkpoint_path):
        try:
            with open(checkpoint_path, 'rb') as f:
                result = pickle.load(f)
            return result
        except:
            # Corrupted checkpoint, will regenerate
            if os.path.exists(checkpoint_path):
                os.remove(checkpoint_path)
    
    try:
        # Create experiment instance with specific grid size and n_agents
        experiment = ControlledMergingExperiment(
            grid_size=task.grid_size,
            n_agents=task.n_agents,
            alpha=task.config.alpha,
            beta=task.config.beta,
            horizon=task.config.horizon
        )
        
        # Generate trial configuration
        np.random.seed(task.trial_seed)
        
        # Generate target trajectory
        target_policy = TargetMovementPolicy(task.grid_size, task.pattern)
        initial_target = np.random.randint(0, task.grid_size[0] * task.grid_size[1])
        
        target_trajectory = [initial_target]
        current_pos = initial_target
        for step in range(task.config.max_steps):
            current_pos = target_policy.get_next_position(current_pos, step)
            target_trajectory.append(current_pos)
        
        # Generate initial agent positions
        total_states = task.grid_size[0] * task.grid_size[1]
        available_positions = list(range(total_states))
        if initial_target in available_positions:
            available_positions.remove(initial_target)
        
        # Handle case where we have more agents than available positions
        if task.n_agents > len(available_positions):
            raise ValueError(f"Cannot place {task.n_agents} agents in grid of size {task.grid_size} with target at {initial_target}")
        
        initial_positions = np.random.choice(
            available_positions, 
            task.n_agents, 
            replace=False
        ).tolist()
        
        # Pre-generate observation random numbers
        observation_randoms = np.random.random((task.config.max_steps, task.n_agents))
        
        # Create trial configuration
        trial_config = {
            'trial_id': task.trial_id,
            'seed': task.trial_seed,
            'target_trajectory': target_trajectory,
            'initial_positions': initial_positions,
            'observation_randoms': observation_randoms,
            'target_pattern': task.pattern,
            'grid_size': task.grid_size,
            'n_agents': task.n_agents
        }
        
        # Run the experiment with MCTS (replacing MPC logic)
        result = experiment._run_single_experiment(
            trial_config,
            task.merge_interval,
            task.config.max_steps,
            task.config.fast_mode,  # FALSE by default
            task.config.random_walk_mode, # Control Mode
            task.merge_method
        )
        
        # Add metadata
        result['task_metadata'] = {
            'grid_size': task.grid_size,
            'n_agents': task.n_agents,
            'pattern': task.pattern,
            'trial_id': task.trial_id,
            'merge_interval': task.merge_interval,
            'merge_method': task.merge_method,
            'trial_seed': task.trial_seed,
            'hostname': socket.gethostname(),
            'pid': os.getpid(),
            'completion_time': datetime.now().isoformat()
        }
        
        # Atomic save to checkpoint
        save_result_atomic(result, checkpoint_path)
        
        # FORCE GARBAGE COLLECTION
        gc.collect()
        
        return result
        
    except Exception as e:
        # Log error but don't crash the worker
        error_msg = f"Task {task.get_task_id()} failed: {str(e)}"
        print(f"ERROR: {error_msg}")
        
        # Save error info
        error_path = checkpoint_path.replace('.pkl', '_ERROR.txt')
        with open(error_path, 'w') as f:
            f.write(f"{datetime.now().isoformat()}: {error_msg}\n")
            f.write(f"Exception type: {type(e).__name__}\n")
            f.write(f"Exception details: {str(e)}\n")
            import traceback
            f.write(f"Traceback:\n{traceback.format_exc()}\n")
        
        return None


def save_result_atomic(result: Dict, filepath: str):
    """Atomically save result to avoid corruption"""
    temp_path = filepath + '.tmp'
    try:
        with open(temp_path, 'wb') as f:
            pickle.dump(result, f)
        
        # Atomic move
        os.rename(temp_path, filepath)
    except Exception as e:
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise e


def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(description='Complete Distributed Belief Merging Experiment')
    parser.add_argument('--config-file', type=str, help='JSON config file path')
    parser.add_argument('--max-workers', type=int, help='Maximum number of workers')
    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints', help='Checkpoint directory')
    parser.add_argument('--results-dir', type=str, default='results', help='Results directory')
    
    args = parser.parse_args()
    
    # Load configuration
    if args.config_file and os.path.exists(args.config_file):
        with open(args.config_file, 'r') as f:
            config_dict = json.load(f)
        config = ExperimentConfig.from_dict(config_dict)
    else:
        # Default configuration with multiple grid sizes and agent numbers
        config = ExperimentConfig(
            grid_sizes=[(10, 10), (20, 20), (30, 30)],  # Multiple grid sizes
            n_agents_list=[2, 3, 4],  # Multiple agent numbers
            alpha=0.1,
            beta=0.2,
            horizon=3,  # Full MCTS horizon (depth)
            n_trials=50,
            max_steps=1000,
            merge_intervals=[0, 10, 25, 50, 100, 200, 500, float('inf')],
            target_patterns=['random', 'evasive', 'patrol'],
            fast_mode=False,  # MCTS
            random_walk_mode=False, # Active search by default
            merge_methods=['standard_kl', 'reverse_kl', 'geometric_mean', 'arithmetic_mean']
        )
    
    print("="*80)
    print("COMPLETE DISTRIBUTED BELIEF MERGING EXPERIMENT")
    print("="*80)
    print(f"Configuration: {config}")
    print(f"Grid sizes: {config.grid_sizes}")
    print(f"Agent numbers: {config.n_agents_list}")
    print(f"MCTS Mode: {not config.fast_mode}")
    print(f"Random Walk Mode: {config.random_walk_mode}")
    print(f"Methods: {config.merge_methods}")
    
    # Create and run experiment manager
    manager = DistributedExperimentManager(
        config=config,
        checkpoint_dir=args.checkpoint_dir,
        results_dir=args.results_dir,
        max_workers=args.max_workers
    )
    
    # Run the experiment
    results = manager.run_distributed_experiment()
    
    if results is not None:
        print("\nExperiment completed successfully!")
        print(f"Results saved in: {manager.results_dir}")
    else:
        print("\nExperiment was interrupted.")


if __name__ == "__main__":
    main()